# -*- coding: utf-8 -*-
"""Yet another copy of LowTraffic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/190yFL7vGgNgLnEualJt7-Je_p1yiXXWt
"""

!pip install transformers torch
!pip install sentence-transformers
!pip install -U accelerate
!pip install -U bitsandbytes

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from sentence_transformers import SentenceTransformer, util

torch.random.manual_seed(0)

# Ensure all tensor operations use the GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
# Ensure the model is initialized to use CPU and the correct data types
model = AutoModelForCausalLM.from_pretrained(
    "NousResearch/Hermes-2-Pro-Mistral-7B",
     device_map="cuda",
    load_in_4bit=True

)
tokenizer = AutoTokenizer.from_pretrained("NousResearch/Hermes-2-Pro-Mistral-7B")



pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
)

model_senttrans = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2').to(device)

# # Load tokenizer and model for embedding generation
# tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-large-en-v1.5')
# model = AutoModel.from_pretrained('BAAI/bge-large-en-v1.5')
# model.eval('cuda')  # Use GPU

# # Define function to convert text to embedding
# def text_to_embedding(text):
#     inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
#     inputs = {k: v.to('cuda') for k, v in inputs.items()}  # Move inputs to GPU
#     with torch.no_grad():
#         outputs = model(**inputs)
#     return outputs.pooler_output[0].cpu().numpy()

# Upload data to Pinecone
#index=pc.Index('syntheticdata')
# for i, row in training_data.iterrows():
#     combined_text = f"Microservice: {row['microservice']}, Timestamp: {row['timestamp']}, Availability: {row['availability_Average']}, Latency Average: {row['latency_Average']}, Latency p50: {row['latency_p50']}, Latency p90: {row['latency_p90']}, Latency p95: {row['latency_p95']}, Latency p99: {row['latency_p99']}, Requests Sum: {row['requests_Sum']}, Issues Injected: {row['issue_injected']}, Path: {row['path']}, Textual Representation: {row['textual_representation']}, Issue Number: {row['IssueNumber']}  "
#     vec = text_to_embedding(combined_text)
#     metadata = {
#         'microservice': row['microservice'],
#         'timestamp': str(row['timestamp']),
#         'availability_Average': row['availability_Average'],
#         'latency_Average': row['latency_Average'],
#         'latency_p50': row['latency_p50'],
#         'latency_p90': row['latency_p90'],
#         'latency_p95': row['latency_p95'],
#         'latency_p99': row['latency_p99'],
#         'requests_Sum': row['requests_Sum'],
#         'issues_injected': row['issue_injected'],
#         'path': row['path'],
#         'textual_representation': row['textual_representation'],
#         'Issue_Number': row['IssueNumber']
#     }
#     index.upsert(vectors=[(str(i), vec, metadata)])

# for i, row in training_data.iterrows():
#     combined_text = f"Microservice: {row['microservice']}, Timestamp: {row['timestamp']}, Availability: {row['availability_Average']}, Latency Average: {row['latency_Average']}, Latency p50: {row['latency_p50']}, Latency p90: {row['latency_p90']}, Latency p95: {row['latency_p95']}, Latency p99: {row['latency_p99']}, Requests Sum: {row['requests_Sum']}"
#     vec = text_to_embedding(combined_text)
#     metadata = {
#         'microservice': row['microservice'],
#         'timestamp': str(row['timestamp']),
#         'availability_Average': row['availability_Average'],
#         'latency_Average': row['latency_Average'],
#         'latency_p50': row['latency_p50'],
#         'latency_p90': row['latency_p90'],
#         'latency_p95': row['latency_p95'],
#         'latency_p99': row['latency_p99'],
#         'requests_Sum': row['requests_Sum']

#     }
#     index.upsert(vectors=[(str(i), vec, metadata)])

import re
import os






def generate_analysis_prompt_extensive():

    prompt = f"""
    Rules of Maintenance :
    Only above 60% is critical
    From 30% to 60%, it is moderate
    Rest is not important

    Dents require contact the manfacturing unit to remove the dent.
    Cracks require to contact the support team to fill up the crack.
    Tear require to contact the support team to remove the tear.

    The predictive maintenance reports show the last maintenance was done 2 weeks before.

    The image shows that there is a dent of 15% damage to the aircraft wings.
    The user wants to know does it require immediate attention and what steps can be taken?
 """
    print(prompt)
    return prompt


def analyze_root_cause():
    """Analyze root cause."""

    prompt = generate_analysis_prompt_extensive()

    print(f"Retrieving and generating response based on the prompt...")
    response = pipe(prompt, max_new_tokens=1000, temperature=0.7, top_p=0.9)[0]['generated_text']
    return prompt,response

def main():
  prompt,response = analyze_root_cause()
  print(response)

if __name__ == "__main__":
    main()